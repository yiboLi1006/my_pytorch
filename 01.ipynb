{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[3, 2, 3],\n        [1, 2, 3],\n        [3, 2, 1]])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建张量\n",
    "t1=torch.zeros(3,2)\n",
    "t2=torch.tensor([[3,2,3],[1,2,3],[3,2,1]])\n",
    "t2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<bound method _TypedStorage.size of  3\n 2\n 3\n 1\n 2\n 3\n 3\n 2\n 1\n[torch.LongStorage of size 9]>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor中的数据时连续分配在内存中，由torch.Storage实例管理\n",
    "t2_storage = t2.storage() # t2的存储实例是一维的连续数组,类似将对应的数组resheap为(-1,1)\n",
    "t2_storage.size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2_storage[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "t2_storage[4] = 2 #通过更改存储的值来更改引用它的张量的内容"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[3, 2, 3],\n        [1, 2, 3],\n        [3, 2, 1]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.int64"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "通过dtype查看数据类型，与numpy一样\n",
    "torch.float32或torch.float —— 32位浮点数\n",
    "torch.float64或torch.double —— 64位双精度浮点数\n",
    "torch.float16或torch.half —— 16位半精度浮点数\n",
    "torch.int8 —— 带符号8位整数\n",
    "torch.uint8 —— 无符号8位整数\n",
    "torch.int16或torch.short —— 带符号16位整数\n",
    "torch.int32或torch.int —— 带符号32位整数\n",
    "torch.int64或torch.long —— 带符号64位整数\n",
    "'''\n",
    "t2.dtype"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# 使用.方法和to()进行类型转换\n",
    "double = torch.zeros(10, 2).to(torch.double)\n",
    "short  = torch.ones(10, 2).to(dtype=torch.short)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.]], dtype=float32)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 与numpy互通\n",
    "\n",
    "# 从points张量创建NumPy数组\n",
    "points = torch.ones(3, 4)\n",
    "points_np = points.numpy()\n",
    "points_np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]], dtype=torch.float64)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NumPy数组创建Tensor张量\n",
    "points_numpy = np.ones((3,4))\n",
    "points_tensor = torch.from_numpy(points_numpy)\n",
    "points_tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "#将其保存到文件中并在某个时候加载回去。杜绝每次开始运行程序时从头开始重新训练模型————将points张量保存到ourpoints.t文件中\n",
    "torch.save(points, './ourpoints.t')   # .t文件只能通过pytorch打开\n",
    "\n",
    "# 另：可将points张量转换为NumPy数组，并将其传递给create_dataset函数来保存points张量\n",
    "import h5py\n",
    "\n",
    "f = h5py.File('./ourpoints.hdf5', 'w')   # 用于创建新的文件对象\n",
    "dset = f.create_dataset('coords', data=points.numpy())\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 4.],\n        [2., 1.],\n        [3., 4.]], device='cuda:0')"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 张量在GPU上运行\n",
    "# 添加device参数\n",
    "points_gpu = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 4.0]], device='cuda')   #如此，将数据存放在本地GPU中\n",
    "points_gpu"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# 也可使用to方法\n",
    "points_gpu = points.to(device='cuda')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "#对于多核GPU，传递从零开始的整数来确定张量分配给哪个GPU\n",
    "points_gpu_to = points.to(device='cuda:0')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "神经网络将浮点表示形式转换为其他浮点表示形式，起始和结束的表示形式通常是可以理解的，但中间表示则不是这样。\n",
    "这些浮点表示存储在张量中。\n",
    "张量是多维数组，它是PyTorch中的基本数据结构。\n",
    "PyTorch有一个全面的标准库，用于张量创建、操作和数学运算。\n",
    "张量可以序列化存储到磁盘上也可以加载回来。\n",
    "PyTorch中的所有张量操作都可以在CPU和GPU上执行，无需更改代码。\n",
    "PyTorch使用结尾下划线标识来表示就地操作（例如Tensor.sqrt_）张量API\n",
    "\n",
    "少量操作仅作为张量对象的方法存在。\n",
    "可通过名称中的下划线来识别，例：zero_，下划线标识表明该方法是就地（inplace）运行的，即直接修改输入而不是创建新的输出并返回。\n",
    "zero_方法会将输入的所有元素清零。任何不带下划线的方法都将保持源张量不变并返回新的张量："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0., 0.],\n        [0., 0.],\n        [0., 0.]])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(3, 2)\n",
    "a.zero_()   # 不保留原有值，就地执行\n",
    "a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tensor 运算"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "t=torch.tensor([[3,-2,3],[-1,2,3],[-3,2,1]])\n",
    "t_=torch.tensor([[1,-3,2],[3,2,1],[2,1,2]])\n",
    "\n",
    "# torch.abs(arg1)\n",
    "# 输入参数：Tensor数据类型的变量\n",
    "# 输出：输入参数的绝对值\n",
    "\n",
    "# torch.add(arg1,arg2)\n",
    "# 输入参数：既可以全部是tensor数据类型的变量，也可以一个是tensor数据类型的变量，另一个是标量\n",
    "# 输出：返回输入参数的求和结果\n",
    "# 与numpy中的矩阵加法运算一样\n",
    "\n",
    "# torch.clamp()\n",
    "# 对输入参数按照自定义的范围进行裁剪，最后将参数裁剪的结果作为输出\n",
    "# 输入参数：torch.clamp(arg,a,b)\n",
    "# 具体裁剪过程：对于张量arg中的元素，小于a的赋值为a,大于b的赋值为b,介于(a，b）的元素保留\n",
    "# 输出：将裁剪结果输出\n",
    "\n",
    "# torch.div(arg,a) /  torch.div(arg1,arg2)\n",
    "# 对张量除标量，或两个张量对应元素相除\n",
    "# 输出：返回输入参数的求商结果\n",
    "\n",
    "# torch.mul()\n",
    "# 求数量积，参数同torch.div()\n",
    "\n",
    "# torch.pow()\n",
    "# 求幂，参数设置同上\n",
    "\n",
    "# torch.mm()\n",
    "# 求积\n",
    "# torch.mm()采用矩阵乘法，被传入的参数被当做矩阵进行处理，参数形状需满足矩阵乘法的条件\n",
    "\n",
    "# torch.mv()\n",
    "# 求积结\n",
    "# torch.mv()运用矩阵与向量之间的乘法规则运算，被传入的参数中第1个参数代表矩阵，第2个采纳数代表向量，顺序不能颠倒。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 自动求导\n",
    "#### 神经网络内部和优化期间的所有操作都是张量之间的操作，而神经网络中的所有参数（例如权重和偏差）也都是张量。掌握如何在张量上执行操作并对其进行有效索引是成功使用PyTorch等工具的关键"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# 使用torch.autograd完成神经网络反向传播中的链式求导\n",
    "# tensor可以记住它们来自什么运算以及其起源的父张量，并且提供相对于输入的导数链。无需手动对模型求导：不管如何嵌套，只要给出前向传播表达式，PyTorch都会自动提供该表达式相对于其输入参数的梯度\n",
    "# 实现auto compute grad 的大致过程：\n",
    "# 1.通过输入的tensor数据类型的变量在神经网络中的前向传播过程生成一张计算图\n",
    "# 2.根据这个计算图和输出结果准确计算出每个参数需要更新的梯度，并通过完成反向传播完成对参数的梯度更新"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 所有PyTorch张量都有一个初始为空的名为grad的属性：\n",
    "torch.autograd.grad is None # True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "conda-env-astro-py",
   "language": "python",
   "display_name": "Python [conda env:astro]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}